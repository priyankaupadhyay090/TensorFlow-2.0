{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLuuVf1Jgf_7"
      },
      "source": [
        "#Sentiment Analysis\n",
        "\n",
        "Sentiment Analysis:\n",
        "\n",
        "**the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.**\n",
        "\n",
        "The example we’ll use here is classifying movie reviews as either postive, negative or neutral.\n",
        "\n",
        "*This guide is based on the following tensorflow tutorial: https://www.tensorflow.org/tutorials/text/text_classification_rnn*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EZ9rj5FgvRe"
      },
      "source": [
        "## IMDB Movie review dataset\n",
        "Well start by loading in the IMDB movie review dataset from keras. This dataset contains 25,000 reviews from IMDB where each one is already preprocessed and has a label as either positive or negative. Each review is encoded by integers that represents how common a word is in the entire dataset. For example, a word encoded by the integer 3 means that it is the 3rd most common word in the dataset.\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rud4Ny7njDQi"
      },
      "source": [
        "### Import and setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F5DInrOgfUw"
      },
      "source": [
        "# imports\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#setup\n",
        "\n",
        "VOCAB_SIZE = 88584 # we have 88584 unique words(least common words) in this dataset\n",
        "MAX_LEN = 250\n",
        "BATCH_SIZE = 64\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLgDVHHojhGJ"
      },
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHouoyywidd_",
        "outputId": "ca0c9698-e628-4a9e-d81e-92001f5cb33f"
      },
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=VOCAB_SIZE)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7npssiTMkRiQ",
        "outputId": "11157b3a-2991-43e1-fceb-c617b4a53b23"
      },
      "source": [
        "# check shape of all train and test dataset\n",
        "print(train_data.shape)\n",
        "print(train_labels.shape) # (25000,) -- 1D with 25000 labels\n",
        "print(test_data.shape)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000,)\n",
            "(25000,)\n",
            "(25000,)\n",
            "(25000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QSQoT64ide2",
        "outputId": "6d93eb04-1f6d-4588-b6f7-ed24caf9f29d"
      },
      "source": [
        "# look at review third\n",
        "\n",
        "train_data[2] # we have different encoding for all the words, like: 106, 14 ......."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 47,\n",
              " 8,\n",
              " 30,\n",
              " 31,\n",
              " 7,\n",
              " 4,\n",
              " 249,\n",
              " 108,\n",
              " 7,\n",
              " 4,\n",
              " 5974,\n",
              " 54,\n",
              " 61,\n",
              " 369,\n",
              " 13,\n",
              " 71,\n",
              " 149,\n",
              " 14,\n",
              " 22,\n",
              " 112,\n",
              " 4,\n",
              " 2401,\n",
              " 311,\n",
              " 12,\n",
              " 16,\n",
              " 3711,\n",
              " 33,\n",
              " 75,\n",
              " 43,\n",
              " 1829,\n",
              " 296,\n",
              " 4,\n",
              " 86,\n",
              " 320,\n",
              " 35,\n",
              " 534,\n",
              " 19,\n",
              " 263,\n",
              " 4821,\n",
              " 1301,\n",
              " 4,\n",
              " 1873,\n",
              " 33,\n",
              " 89,\n",
              " 78,\n",
              " 12,\n",
              " 66,\n",
              " 16,\n",
              " 4,\n",
              " 360,\n",
              " 7,\n",
              " 4,\n",
              " 58,\n",
              " 316,\n",
              " 334,\n",
              " 11,\n",
              " 4,\n",
              " 1716,\n",
              " 43,\n",
              " 645,\n",
              " 662,\n",
              " 8,\n",
              " 257,\n",
              " 85,\n",
              " 1200,\n",
              " 42,\n",
              " 1228,\n",
              " 2578,\n",
              " 83,\n",
              " 68,\n",
              " 3912,\n",
              " 15,\n",
              " 36,\n",
              " 165,\n",
              " 1539,\n",
              " 278,\n",
              " 36,\n",
              " 69,\n",
              " 44076,\n",
              " 780,\n",
              " 8,\n",
              " 106,\n",
              " 14,\n",
              " 6905,\n",
              " 1338,\n",
              " 18,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 610,\n",
              " 40,\n",
              " 6,\n",
              " 87,\n",
              " 326,\n",
              " 23,\n",
              " 2300,\n",
              " 21,\n",
              " 23,\n",
              " 22,\n",
              " 12,\n",
              " 272,\n",
              " 40,\n",
              " 57,\n",
              " 31,\n",
              " 11,\n",
              " 4,\n",
              " 22,\n",
              " 47,\n",
              " 6,\n",
              " 2307,\n",
              " 51,\n",
              " 9,\n",
              " 170,\n",
              " 23,\n",
              " 595,\n",
              " 116,\n",
              " 595,\n",
              " 1352,\n",
              " 13,\n",
              " 191,\n",
              " 79,\n",
              " 638,\n",
              " 89,\n",
              " 51428,\n",
              " 14,\n",
              " 9,\n",
              " 8,\n",
              " 106,\n",
              " 607,\n",
              " 624,\n",
              " 35,\n",
              " 534,\n",
              " 6,\n",
              " 227,\n",
              " 7,\n",
              " 129,\n",
              " 113]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ8NIrf0idil",
        "outputId": "9b578ea5-07d1-472e-d255-9871a18f2b06"
      },
      "source": [
        "# also length of all reviews are not same\n",
        "print(len(train_data[0]))\n",
        "print(len(train_data[2]))\n",
        "print(len(train_data[0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "218\n",
            "141\n",
            "218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfWGuBcXlocT"
      },
      "source": [
        "###More Preprocessing\n",
        "If we have a look at some of our loaded in reviews, we'll notice that they are different lengths. This is an issue. We cannot pass different length data into our neural network. Therefore, we must make each review the same length. To do this we will follow the procedure below:\n",
        "- if the review is greater than 250 words then trim off the extra words\n",
        "- if the review is less than 250 words add the necessary amount of 0's **using padding** to make it equal to 250.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eGevuUSidjd"
      },
      "source": [
        "train_data = sequence.pad_sequences(train_data, MAX_LEN)\n",
        "test_data = sequence.pad_sequences(test_data, MAX_LEN)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVcScNsfidn1",
        "outputId": "2d2adb03-ea1a-4b0c-c0de-f0070369d7e9"
      },
      "source": [
        "# now check revie 3rd agai which lenght was 141 so Keras must have added 0's padding on left side of interger\n",
        "\n",
        "train_data[2]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     1,    14,    47,     8,    30,    31,     7,     4,\n",
              "         249,   108,     7,     4,  5974,    54,    61,   369,    13,\n",
              "          71,   149,    14,    22,   112,     4,  2401,   311,    12,\n",
              "          16,  3711,    33,    75,    43,  1829,   296,     4,    86,\n",
              "         320,    35,   534,    19,   263,  4821,  1301,     4,  1873,\n",
              "          33,    89,    78,    12,    66,    16,     4,   360,     7,\n",
              "           4,    58,   316,   334,    11,     4,  1716,    43,   645,\n",
              "         662,     8,   257,    85,  1200,    42,  1228,  2578,    83,\n",
              "          68,  3912,    15,    36,   165,  1539,   278,    36,    69,\n",
              "       44076,   780,     8,   106,    14,  6905,  1338,    18,     6,\n",
              "          22,    12,   215,    28,   610,    40,     6,    87,   326,\n",
              "          23,  2300,    21,    23,    22,    12,   272,    40,    57,\n",
              "          31,    11,     4,    22,    47,     6,  2307,    51,     9,\n",
              "         170,    23,   595,   116,   595,  1352,    13,   191,    79,\n",
              "         638,    89, 51428,    14,     9,     8,   106,   607,   624,\n",
              "          35,   534,     6,   227,     7,   129,   113], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtVsphrMmlVm"
      },
      "source": [
        "## Create Model\n",
        "\n",
        "We'll use a **word embedding layer** as the first layer in our model and add a **LSTM layer** afterwards that feeds into **one dense node** to get our predicted sentiment. \n",
        "\n",
        "32 stands for the output dimension of the vectors generated by the embedding layer. We can change this value if we'd like!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raeHQ7eemOph"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "              tf.keras.layers.Embedding(VOCAB_SIZE, 32), # 32 dimension, passing interger to word embeddings layer to get more meaningful representation\n",
        "              tf.keras.layers.LSTM(32), # LSTM will creat 32 dimension for each words\n",
        "              tf.keras.layers.Dense(1, activation='sigmoid') # predict sentiment number < 0.5--> negative review, num> 0.5--> positive review\n",
        "            ])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIXnfLAcmOqq",
        "outputId": "de52ec8f-f64f-447d-f4ff-ea7475243a24"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 32)          2834688   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                8320      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsQ-sMWPmOum"
      },
      "source": [
        "# compile the model\n",
        "\n",
        "model.compile(loss= 'binary_crossentropy', optimizer= 'rmsprop', metrics=['acc'])\n",
        "\n",
        "# 'binary_crossentropy' --> can tell how far away we are from right probability "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WKu70mmoZf2"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj2SH8ATmOwV",
        "outputId": "1fd7899e-fef5-4df1-ca5e-8514eb9f9a54"
      },
      "source": [
        "model_train = model.fit(train_data, train_labels, epochs = 10, validation_split= 0.2)\n",
        "\n",
        "\n",
        "# validation_split= 0.2 -- use 20% data from train for model evaluation"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 42s 59ms/step - loss: 0.4256 - acc: 0.8044 - val_loss: 0.3105 - val_acc: 0.8702\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 36s 57ms/step - loss: 0.2359 - acc: 0.9107 - val_loss: 0.2937 - val_acc: 0.8784\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 35s 57ms/step - loss: 0.1819 - acc: 0.9347 - val_loss: 0.3407 - val_acc: 0.8778\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 36s 57ms/step - loss: 0.1503 - acc: 0.9471 - val_loss: 0.3045 - val_acc: 0.8818\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 35s 57ms/step - loss: 0.1276 - acc: 0.9549 - val_loss: 0.3250 - val_acc: 0.8844\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 35s 57ms/step - loss: 0.1109 - acc: 0.9631 - val_loss: 0.3028 - val_acc: 0.8960\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.0974 - acc: 0.9664 - val_loss: 0.3453 - val_acc: 0.8848\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 35s 57ms/step - loss: 0.0844 - acc: 0.9736 - val_loss: 0.3597 - val_acc: 0.8762\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 35s 57ms/step - loss: 0.0774 - acc: 0.9747 - val_loss: 0.3612 - val_acc: 0.8836\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 36s 57ms/step - loss: 0.0670 - acc: 0.9785 - val_loss: 0.3479 - val_acc: 0.8854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3N-vb_jqhIr"
      },
      "source": [
        "# Model Evaluation\n",
        "- with test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jBEcC2emOzq",
        "outputId": "e54eff03-f91d-4806-8d20-22916a0a7ee2"
      },
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 13s 17ms/step - loss: 0.4263 - acc: 0.8584\n",
            "[0.42625370621681213, 0.8583999872207642]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpHI4pYhq-Zz"
      },
      "source": [
        "# Making predictions\n",
        "\n",
        "let’s use our network to make predictions on our own reviews. \n",
        "\n",
        "Since our reviews are encoded well need to convert any review that we write into that form so the network can understand it. To do that well load the encodings from the dataset and use them to encode our own data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVitVfy_q9l-",
        "outputId": "00299c28-603d-4945-ca7c-35156cc2a6de"
      },
      "source": [
        "# Encoding:  text/word into integer \n",
        "\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "def encode_text(text):\n",
        "  tokens = tf.keras.preprocessing.text.text_to_word_sequence(text) # convert text into tokens(word sequences)\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  return sequence.pad_sequences([tokens], MAX_LEN)[0] # first index to padded\n",
        "\n",
        "text = \"that movie was just amazing, so amazing\" # 7 words, we get 7 integer encoding at the end of list : [0 0 0 ....... 12  17  13  40 477  35 477]\n",
        "encoded = encode_text(text)\n",
        "print(encoded)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCXXVbdvq9rG",
        "outputId": "a0a3a6cb-ddf9-4c86-d45c-2a9b03db18c1"
      },
      "source": [
        "# Decoding: integer to word\n",
        "\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "    PAD = 0 # no word there\n",
        "    text = \"\"\n",
        "    for num in integers:\n",
        "      if num != PAD:\n",
        "        text += reverse_word_index[num] + \" \"\n",
        "\n",
        "    return text[:-1]\n",
        "  \n",
        "print(decode_integers(encoded))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "that movie was just amazing so amazing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iKtyqpC9UHA",
        "outputId": "191fa304-8c9c-4631-d1cc-5e680920bf28"
      },
      "source": [
        "# now time to make a prediction\n",
        "\n",
        "def predict(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1,250))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred) \n",
        "  print(result[0])\n",
        "\n",
        "positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
        "predict(negative_review)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8896567]\n",
            "[0.40000808]\n"
          ]
        }
      ]
    }
  ]
}