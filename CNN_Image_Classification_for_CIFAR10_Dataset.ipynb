{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CNN_Image_Classification_for_CIFAR10_Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqLsm2XzNQSE"
      },
      "source": [
        "##Creating a Convnet\n",
        "\n",
        "###CIFAR 10 Dataset\n",
        "The problem we will consider here is classifying 10 different everyday objects. It contains 60,000 32x32 color images with 6000 images of each class. \n",
        "\n",
        "The labels in this dataset are the following:\n",
        "- Airplane\n",
        "- Automobile\n",
        "- Bird\n",
        "- Cat\n",
        "- Deer\n",
        "- Dog\n",
        "- Frog\n",
        "- Horse\n",
        "- Ship\n",
        "- Truck\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0O-C6gcXAyR"
      },
      "source": [
        "# Setup and import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB9ifo79W92a"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdM14MwsXbGj"
      },
      "source": [
        "# Load and split dataset into train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIw4lgoZW-BE",
        "outputId": "6ca775bb-b476-40a9-d17b-398410043d28"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n",
            "170508288/170498071 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPrAGqjlW-B5"
      },
      "source": [
        "## Normalizing Train and test images Pixel values between 0 and 1\n",
        "\n",
        "train_images, test_images  = train_images/255.0, test_images/255.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNxhImukW-Gq"
      },
      "source": [
        "## all the class/labels name\n",
        "\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "4wasxnn0W-Hc",
        "outputId": "0fbeeb59-cb7c-4289-843e-8b80a6cbc320"
      },
      "source": [
        "# check some images\n",
        "\n",
        "IMG_INDEX = 12\n",
        "\n",
        "plt.imshow(train_images[IMG_INDEX], cmap = plt.cm.binary)\n",
        "plt.xlabel(class_names[train_labels[IMG_INDEX][0]], color = 'r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnVzK--Ec5aW"
      },
      "source": [
        "# Model create\n",
        "\n",
        "##CNN Architecture\n",
        "A common architecture for a CNN is a stack of Conv2D and MaxPooling2D layers followed by a few denesly connected layers. To idea is that the stack of convolutional and maxPooling layers extract the features from the image. Then these features are flattened and fed to densly connected layers that determine the class of an image based on the presence of features.\n",
        "\n",
        "We will start by building the **Convolutional Base**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVvtpzccfrYU"
      },
      "source": [
        "**Layer 1**\n",
        "\n",
        "The input shape of our data will be 32, 32, 3 and we will process 32 filters of size 3x3 over our input data. We will also apply the activation function relu to the output of each convolution operation.\n",
        "\n",
        "**Layer 2**\n",
        "\n",
        "This layer will perform the max pooling operation using 2x2 samples and a stride of 2.\n",
        "\n",
        "**Other Layers**\n",
        "\n",
        "The next set of layers do very similar things but take as input the feature map from the previous layer. They also increase the frequency of filters from 32 to 64. We can do this as our data shrinks in spacial dimensions as it passed through the layers, meaning we can afford (computationally) to add more depth (more filter to take out more features)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6e0OoVmW-M9"
      },
      "source": [
        "# use 3 convo filter and 2 max polling layers \n",
        "\n",
        "cnn_model = models.Sequential()\n",
        "\n",
        "# Convolution filter of size 3x3 and number fo filters = 32, 64, 64\n",
        "# activation = Relu\n",
        "# imput image shape = 32x32x3\n",
        "## so model.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "\n",
        "cnn_model.add(layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (32,32,3)))\n",
        "\n",
        "# # max polling filter with size 2x2\n",
        "cnn_model.add(layers.MaxPool2D((2,2)))\n",
        "\n",
        "cnn_model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
        "cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
        "cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pGROhqpW-St",
        "outputId": "1b10a4f4-09b0-4506-9a96-148ef6474978"
      },
      "source": [
        "# model summary\n",
        "\n",
        "cnn_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 56,320\n",
            "Trainable params: 56,320\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUdMtSjsgXS8"
      },
      "source": [
        "- conv2d_2 (Conv2D)--> (None, 30, 30, 32) --> output shape is 30x30 instead of 32x32 because we do samplinh wihtout padding so we get 2 pixel less\n",
        "\n",
        "- also notice that depth of our image increases but the spacial dimensions reduce drastically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tCoxof9hfJU"
      },
      "source": [
        "## Classifier layer: Adding Dense Layers\n",
        "So far, we have just completed the **convolutional base**. Now we need to take these extracted features and add a way to classify them. This is why we add the following layers to our model.\n",
        "\n",
        "- at the end we have classfier layer (so output of last convo layer will be input of classifier layer)-->  output layer will be fully connected/dense layers \n",
        "- we will flatten the output of last convo layer and pass it into classifier (which classfier use, will depend on which loss we are using)\n",
        "- and activation = 'softmax' to get probability of all classes \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1oGIqhyhd9Y"
      },
      "source": [
        "cnn_model.add(layers.Flatten())\n",
        "\n",
        "## classfier layers-- will use 2 fully connected layers \n",
        "# first fully connected layer\n",
        "cnn_model.add(layers.Dense(64, activation='relu')) # 64 neurons \n",
        "\n",
        "# 2nd fully connected layer \n",
        "cnn_model.add(layers.Dense(10)) # 10 neurons as 10 classes "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2gqbGzUhAQ2",
        "outputId": "cba69e0f-2b23-4853-9504-e73473c7261b"
      },
      "source": [
        "cnn_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                65600     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 122,570\n",
            "Trainable params: 122,570\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x4GRLe0lGJf"
      },
      "source": [
        "We can see that the flatten layer changes the shape of our data so that we can feed it to the 64-node dense layer, follwed by the final output layer of 10 neurons (one for each class)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0NDC7GulpKc"
      },
      "source": [
        "## Compile the model with hyprparamterer: loss, optimizer, performance metrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0saWlMNlnvA"
      },
      "source": [
        "cnn_model.compile(optimizer= 'adam',\n",
        "                  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics= ['accuracy'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StSenEzNk7ck"
      },
      "source": [
        "# Train CNN model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAgzs-fqW-YX",
        "outputId": "ba545e89-5253-4791-d9a4-e0b1e6eb2963"
      },
      "source": [
        "# train \n",
        "\n",
        "model_training = cnn_model.fit(train_images, train_labels,epochs=10, batch_size=50, validation_data=(test_images, test_labels))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 22s 10ms/step - loss: 1.5539 - accuracy: 0.4351 - val_loss: 1.3311 - val_accuracy: 0.5224\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 1.2015 - accuracy: 0.5741 - val_loss: 1.1239 - val_accuracy: 0.5959\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 1.0524 - accuracy: 0.6314 - val_loss: 1.0204 - val_accuracy: 0.6405\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.9475 - accuracy: 0.6671 - val_loss: 0.9573 - val_accuracy: 0.6649\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.8737 - accuracy: 0.6948 - val_loss: 0.9003 - val_accuracy: 0.6832\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.8155 - accuracy: 0.7157 - val_loss: 0.9121 - val_accuracy: 0.6896\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.7640 - accuracy: 0.7326 - val_loss: 0.8746 - val_accuracy: 0.6962\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.7235 - accuracy: 0.7472 - val_loss: 0.8472 - val_accuracy: 0.7055\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.6889 - accuracy: 0.7583 - val_loss: 0.8695 - val_accuracy: 0.7002\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.6498 - accuracy: 0.7726 - val_loss: 0.8720 - val_accuracy: 0.7168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWbDE1Tc5lxd"
      },
      "source": [
        "# Model Evalution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xii3aasiW-c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "923e0ba2-4457-499e-f6b8-e4355380d972"
      },
      "source": [
        "test_loss, test_acc = cnn_model.evaluate(test_images, test_labels, verbose=2)\n",
        "print('Test accuracy: ', test_acc)\n",
        "\n",
        "\n",
        "## acc = 71%\n",
        "# This accuracy isn't bad for a simple model like this, but we'll dive into some better approaches for computer vision below."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.8720 - accuracy: 0.7168 - 1s/epoch - 5ms/step\n",
            "Test accuracy:  0.7167999744415283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxmrUWjw-JCm"
      },
      "source": [
        "# How to work for small dataset\n",
        "\n",
        "- check Data_Agumentation.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKPLa2YaW-th"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKuD9Qm8W-uf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}