# -*- coding: utf-8 -*-
"""Logistic_Regression_for_binary_clasification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tkExcwIH4S_cjrfTMQve2MJGeIdTcIVE

```Use case/Task: on the given of Titanic dataset, predict the liklihood of person to survive in the future```





# Setup and import
"""

!pip install -q sklearn

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

from __future__ import absolute_import, division, print_function, unicode_literals

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import clear_output # to clear output
from six.moves import urllib

import tensorflow as tf
import tensorflow.compat.v2.feature_column as fc # to create feature columns

"""# Titanic dataset

Major part of machine learning is data. It's so important to exploring, cleaning and selecting appropriate data.



**Let's load a dataset and learn how to  explore it using some built-in tools. **
"""

# load dataset

data_train = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')
data_eval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # test data

"""####  Training vs Testing Data
there are **two different datasets** above. This is because when we train models, we need two sets of data: **training and testing**. 

The **training** data is what we feed to the model so that it can develop and learn. It is usually a much larger size than the testing data.

The **testing** data is what we use to evaulate the model and see how well it is performing. We must use a seperate set of data that the model has not been trained on to evaluate it. Can you think of why this is?

Well, the point of our model is to be able to make predictions on NEW data, data that we have never seen before. If we simply test the model on the data that it has already seen we cannot measure its accuracy accuratly. We can't be sure that the model hasn't simply memorized our training data. This is why we need our testing and training data to be seperate.


"""

# data analysis

#1. see dataset
print(data_train.head(5))

#2 see all the columns
print("All Columns: ",data_train.columns)

#3. all info
print(data_train.info())

#4. mean, standar deviation, maximum, minimum
print(data_train.describe())

# assigning column to Lable Y

y_train = data_train.pop('survived') # popping this column and assigning it as Lable (we just can't do data_train['survived'] becase these way this couls will also be part of features and Y, which is wrong so we have to pop it out)
y_eval = data_eval.pop('survived')

y_train.head(5)

data_train.shape # (627, 9)

# 627 = rows/observations, 9 = features

# data plot with age column

print(data_train.age) # this can give age column value
data_train.age.hist(bins=20,color='R')

# sex data

print(data_train.sex.value_counts())
data_train.sex.value_counts().plot(kind='barh')

# we can also do this
data_train.sex.hist(bins=10)

# class columns

data_train['class'].value_counts().plot(kind='barh')

# % survival by sex

pd.concat([data_train,y_train], axis=1).groupby('sex').survived.mean().plot(kind='barh').set_xlabel('% survive')

"""after analyze:

- majority of passenger are in their 20's or 30's
- majority of passenger are male
- majority of passenger are in the 'third' class
- females have a much higher chance to survive




"""

# for male
mask1 = data_train.sex == 'male'
# mask2 = data_train.sex == 'female'
mask3 = data_train.age.between(20, 30)
mask4 = data_train['class'] == 'Third'


male_filtered_df = data_train.loc[mask1 & mask3 & mask4, ['sex', 'age', 'class']]
male_filtered_df.head(5)

# for female

mask2 = data_train.sex == 'female'
mask3 = data_train.age.between(20, 30)
mask4 = data_train['class'] == 'Third'


female_filtered_df = data_train.loc[mask2 & mask3 & mask4, ['sex', 'age', 'class']]
female_filtered_df.head(5)

"""# Feature Columns
```convert categorcial value into numerical value```


In feature columns we have two different kinds of information: Categorical and Numeric

Our categorical data is anything that is not numeric! For example, the sex column does not use numbers, it uses the words "male" and "female".

Before we continue and create/train a model we must convet our categorical data into numeric data. We can do this by encoding each category with an integer (ex. male = 1, female = 2).

TensorFlow has some tools to help - 
1. ```tf.feature_column.categorical_column_with_vocabulary_list() method```
2. ```tf.feature_column.numeric_column()```
"""

categorical_col = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck','embark_town', 'alone']
numerical_col = ['age','fare']


feature_col = []
for each_feature_name in categorical_col:
  vocab = data_train[each_feature_name].unique() # get a list of all feature one by one
  feature_col.append(tf.feature_column.categorical_column_with_vocabulary_list(each_feature_name, vocab))


for each_feature_name in numerical_col:
  feature_col.append(tf.feature_column.numeric_column(each_feature_name, dtype= tf.float32))


print(feature_col)

"""# Model

## The Training Process
How our model is trained. Specifically, how input data is fed to our model. 

For this specific model data is going to be streamed into it in small batches of 32. This means we will not feed the entire dataset to our model at once, but simply small batches of entries. We will feed these batches to our model multiple times according to the number of **epochs**. 

An **epoch** is simply one stream of our entire dataset. The number of epochs we define is the amount of times our model will see the entire dataset. We use multiple epochs in hope that after seeing the same data multiple times the model will better determine how to estimate it.

Example- if we have 10 ephocs, our model will see the same dataset 10 times. 

Since we need to feed our data in batches and multiple times, we need to create something called an **input function**. The input function simply defines how our dataset will be converted into batches at each epoch.

#### Input Function 
- define how batches and epoch are gloing to feed into model.

- The TensorFlow model we are going to use requires that the data we pass it comes in as a ```tf.data.Dataset``` object. This means we must create a *input function* that can convert our current pandas dataframe into that object. 

help: https://www.tensorflow.org/tutorials/estimator/linear
"""

def make_input_function(data_df, label_df, num_of_epochs = 10, shuffle= True, batch_size = 32): # shuffle= True: shuffle/randomize the data value before pass it to model
  def input_function():
    data_set = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df)) # create a dict of all feature of data_df --> and then pass it to tf.data.Dataset : which create a object with data(data_df) with it's label_df

    if shuffle:
      data_set = data_set.shuffle(1000)
    
    data_set = data_set.batch(batch_size).repeat(num_of_epochs)     # split dataset into batches of 32 and repeat process for number of epochs
    return data_set     # return a batch of the dataset

  return input_function # # return a function object 


train_input_func = make_input_function(data_train, y_train)   # calling input_function to give a dataset object, which we can use for Model
eval_input_func = make_input_function(data_eval, y_eval, num_of_epochs=1, shuffle=False) # we are overriding num_of_epochs=1, shuffle=False here (whereas for train_input_func, epoch (10) and shuffle(true) will be dafult which are passed into function)

"""# Model Create

- using linear estimator for linear regression
"""

# create Linear estimator

linear_estimator = tf.estimator.LinearClassifier(feature_columns= feature_col)

"""# Model Training"""

# Train
linear_estimator.train(train_input_func)

# result accuracy on test data: get model metrics/stats
result = linear_estimator.evaluate(eval_input_func)


# clear console output
clear_output()

# print MSE: the result variable is simply a dict of stats about our model
print(result['accuracy'])# 0.7613636 --> 76.13% accuracy

"""# Prediction

Now we can actually use this model to make predicitons.

We can use the ```.predict()``` method to get survival probabilities from the model. This method will return a list of dicts that store a predicition for each of the entries in our testing data set. Below we've used some pandas magic to plot a nice graph of the predictions.

As you can see the survival rate is not very high :/
"""

# create dict
prediction_dict= list(linear_estimator.predict(eval_input_func))
# print(prediction_dict)
predict_prob = pd.Series([pred['probabilities'][1] for pred in prediction_dict])
predict_prob.plot(kind = 'hist', bins=20, title='Prediction Probibilites')

