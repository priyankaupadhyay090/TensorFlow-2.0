# -*- coding: utf-8 -*-
"""Pre_Trained_CNN_Image_Classification_with_Fine_tuning_for_small_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1haeXByqhXaaGje8UxTxQow6fYsrhvvnC

# How to work for small dataset (cifar 60000 is still small dataset, CNN is trained on millions images)
In the situation where you don't have millions of images it is difficult to train a CNN from scratch that performs very well. This is why we will learn about a few techniques we can use to train CNN's on small datasets of just a few thousand images. 
- 1. **Data agumentation** to avoid overfitting and increase number of dataset --> check CNN_Deep_Learning.ipynb
- 2. **Pre-trained model** if both dataset are similar with  **Fine tunning** by using transfer learning from one model (trained on largse dataset) to new model(which has only small dataset)

##2. **Pre-trained Models**
You would have noticed that the model above takes a few minutes to train in the NoteBook and only gives an accuaracy of ~70%. This is okay but surely there is a way to improve on this. 

We can also use a pretrained CNN as apart of our own custom network to improve the accuracy of our model. We know that CNN's alone (with no dense layers) don't do anything other than map the presence of features from our input. This means we can use a pretrained CNN, one trained on millions of images, as the start of our model. This will allow us to have a very good convolutional base before adding our own dense layered classifier at the end. In fact, by using this techique we can train a very good classifier for a realtively small dataset (< 10,000 images). This is because the convnet already has a very good idea of what features to look for in an image and can find them very effectively. So, if we can determine the presence of features all the rest of the model needs to do is determine which combination of features makes a specific image.



### **Fine Tuning**
When we employ the pre-trained technique, we will often want to tweak the final layers in our convolutional base to work better for our specific problem. This involves not touching or retraining the earlier layers in our convolutional base but only adjusting the final few. We do this because the first layers in our base are very good at extracting low level features lile lines and edges, things that are similar for any kind of image. Where the later layers are better at picking up very specific features like shapes or even eyes. If we adjust the final layers than we can look for only features relevant to our very specific problem.

# Using a Pretrained Model
- use a pretrained model and fine tuning to classify images of dogs and cats using a small dataset.

- help:  https://www.tensorflow.org/tutorials/images/transfer_learning*

# setup and imports
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

import tensorflow as tf
from tensorflow.keras import datasets, layers, models

import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
keras = tf.keras

"""## dataset
- Load the *cats_vs_dogs* dataset from the modoule tensorflow_datatsets.

- This dataset contains (image, label) pairs where images have different spatial dimensions than CIFAR 10(32x32) and 3 color channels.


"""

import tensorflow_datasets as tfds
tfds.disable_progress_bar()


# split dataset into train, validation and test

(raw_train, raw_validation, raw_test), metadata = tfds.load('cats_vs_dogs', split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'], 
                                                            with_info=True, as_supervised=True ) # train 0-80%, validation- 80% to 90%, test = 90% to 100%

# creates a function object that we can use to get labels

get_label_name = metadata.features['label'].int2str  


# display 5 images from the dataset
for image, label in raw_train.take(5): # image 0, 1,2,3,4
  plt.figure()
  plt.imshow(image)
  plt.title(get_label_name(label), color ='r')

"""###Data Preprocessing
Since the sizes of our images are all different, we need to convert them all to the same size. 


"""

img_size = 160 # All images will be resized to 160x160

def resize_format(image, label):
   '''
   return an image that is reshaped with img_size
   '''

   image = tf.cast(image, tf.float32) # converting pixel value into float, bcz we need to divide with float number 127.5
   image = (image/127.5) - 1
   image = tf.image.resize(image, (img_size, img_size))
   return image, label

# passing the above image size into all images --> with map() method

train = raw_train.map(resize_format)
validation = raw_validation.map(resize_format)
test = raw_test.map(resize_format)

for image, label in train.take(3):
  plt.figure()
  plt.imshow(image, cmap= plt.cm.binary)
  plt.title(get_label_name(label), color = 'r')

# shuffle the train images 

BATCH_SIZE = 32
SHUFFLE_BUFFER_SIZE = 1000

train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
validation_batches = validation.batch(BATCH_SIZE)
test_batches = test.batch(BATCH_SIZE)


# just too check original and new img shape

for img, label in raw_train.take(3): # images are in diferent size
  print('original shape', img.shape)


for img, label in train.take(3): # all images are in same size now
  print('original shape', img.shape)

"""###Picking a Pretrained Model
The model we are going to use as the convolutional base for our model is the **MobileNet V2** developed at Google. This model is trained on 1.4 million images and has 1000 different classes.

We want to use this model but only its convolutional base. So, when we load in the model, we'll specify that we don't want to load the top (classification) layer. We'll tell the model what input shape to expect and to use the predetermined weights from *imagenet* (Googles dataset).


"""

IMG_SHAPE = (img_size, img_size, 3) # img_size = 160 defined above

# base model creation with Imagenet dataset using MobileNetV2 architecture  

base_model = tf.keras.applications.MobileNetV2(input_shape= IMG_SHAPE, include_top=False,
                                               weights='imagenet')

base_model.summary()

"""At this point this base_model will simply give output of a shape (32, 5, 5, 1280) tensor that is a feature extraction from our original (1, 160, 160, 3) image -->1 means 1 full image. The 32 means that we have 32 layers of differnt filters/features."""

for image, _ in train_batches.take(1):
   pass

feature_batch = base_model(image)
print(feature_batch.shape)

"""###Freezing the Base
The term **freezing** refers to disabling the training property of a layer. It simply means we wonâ€™t make any changes to the weights of any layers that are frozen during training. This is important as we don't want to change the convolutional base that already has learned weights.


"""

base_model.trainable = False

base_model.summary()


# now we have Trainable params: 0 --> no traing time time as base model has been already trained

"""###Adding our Classifier
Now that we have our base layer setup, we can add the classifier. Instead of flattening the feature map (5,5,1280) of the base layer we will use a global average pooling layer that will average the entire 5x5 area of each 2D feature map and return to us a single 1280 element vector per filter.  


"""

# take global average pooling from each 1280 layers from each filter(total 32 filter) which has 5*5 image shape and convert into 1D as flatten

global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

"""Finally, we will add the predicition layer that will be a single dense neuron. We can do this because we only have two classes to predict for.



"""

prediction_layer = keras.layers.Dense(1)

"""Now we will combine these layers together in a model.



"""

model = tf.keras.Sequential([ base_model, global_average_layer, prediction_layer])

model.summary()

"""###Training the Model
Now we will train and compile the model. We will use a very small learning rate to ensure that the model does not have any major changes made to it.
"""

base_learning_rate = 0.0001
model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

# We can evaluate the model right now to see how it does before training it on our new images
initial_epochs = 20
validation_steps=20

loss0,accuracy0 = model.evaluate(validation_batches, steps = validation_steps)

# Now we can train it on our train and validation images batch
model_train = model.fit(train_batches,
                    epochs=initial_epochs,
                    validation_data=validation_batches)

acc = model_train.history['accuracy'] # saving accuracy as a list from all epochs
print(acc)

model.save("dogs_vs_cats.h5")  # we can save the model and reload it at anytime in the future
new_model = tf.keras.models.load_model('dogs_vs_cats.h5')

"""##Object Detection
If you'd like to learn how you can perform object detection and recognition with tensorflow check out the guide below.

https://github.com/tensorflow/models/tree/master/research/object_detection
"""

